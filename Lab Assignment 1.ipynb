{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 1\n",
    "\n",
    "IE7300 - Statistical Learning for Engineerings\n",
    "\n",
    "Name: Dat H. Tran\n",
    "\n",
    "SID: 002925316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thund\\AppData\\Local\\Temp\\ipykernel_47108\\3902267410.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = pandas.read_csv(\"housing.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = pandas.read_csv(\"yachtData.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "D3 = pandas.read_csv(\"concreteData.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy.ones(D1.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class LRMode(enum.Enum):\n",
    "    CLOSED_FORM_SOLUTION = 0\n",
    "    GRADIENT_DESCENT = 1,\n",
    "    STOCHASTIC_GRADIENT_DESCENT = 2\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, mode:LRMode = LRMode.CLOSED_FORM_SOLUTION, train_test_split = 0.2, regularization = 0, learning_rate = 0.01, n_iteration = 10000, tolerance = 0, minibatch = 32) -> None:\n",
    "        # closed form solution by default\n",
    "        self.mode = mode\n",
    "        # Default split is 20% test\n",
    "        self.train_test_split = train_test_split\n",
    "        # Choose a positive value if regularization is desired\n",
    "        self.regularization = max(regularization, 0)\n",
    "        # Has no effect if the model is fit using closed form solution (mode = \"closed_form\")\n",
    "        self.learning_rate = learning_rate\n",
    "        # Should not exceed 50000. Has no effect if the model is fit using closed form solution (mode = \"closed_form\")\n",
    "        self.n_iteration = min(n_iteration, 50000)\n",
    "        # Choose a positive value to specify additional stopping condition for the training process. If the difference in root mean squared error between iteration is less than this value, the training stop. Has no effect if the model is fit using closed form solution (mode = \"closed_form\")\n",
    "        self.tolerance = max(tolerance, 0)\n",
    "        # Size of the minibatch to be used in SGD\n",
    "        self.minibatch = max(minibatch, 1)\n",
    "\n",
    "    def standardize(self, x:pandas.DataFrame) -> pandas.DataFrame:\n",
    "        # store mean and std to be used as rescaling parameter for predict and testing\n",
    "        self.nor_mean = x.mean()\n",
    "        self.nor_std = x.std()\n",
    "        return (x-self.nor_mean)/self.nor_std\n",
    "    \n",
    "    def RMSE(self, x:pandas.DataFrame):\n",
    "        # split the target column\n",
    "        test_x = x.iloc[:, 0:x.shape[1]-1]\n",
    "        test_y = x.iloc[:, x.shape[1]-1]\n",
    "\n",
    "        y_pred = numpy.dot(test_x, self.w)\n",
    "        residuals = y_pred - test_y\n",
    "        \n",
    "        return numpy.sqrt(numpy.sum((residuals ** 2)) / test_x.shape[0])\n",
    "    \n",
    "    def SSE(self, x:pandas.DataFrame):\n",
    "        # split the target column\n",
    "        test_x = x.iloc[:, 0:x.shape[1]-1]\n",
    "        test_y = x.iloc[:, x.shape[1]-1]\n",
    "\n",
    "        y_pred = numpy.dot(test_x, self.w)\n",
    "        residuals = y_pred - test_y\n",
    "        \n",
    "        return numpy.sum((residuals ** 2))\n",
    "\n",
    "\n",
    "    def fit(self, d:pandas.DataFrame):\n",
    "        # d is assumed to be a DataFrame object, contains all features and the target column as the last column\n",
    "        # Store a shuffled copy of the input dataset\n",
    "        self.D = d.sample(frac=1).copy()\n",
    "\n",
    "        # Split data into train and test set\n",
    "        t_size = int(self.D.shape[0] * self.train_test_split)\n",
    "        self.test = self.D.iloc[0:t_size]\n",
    "        self.train = self.D.iloc[t_size:]\n",
    "\n",
    "        # standardize\n",
    "        self.train = self.standardize(self.train)\n",
    "        # standardize the test set using mean and std of the train set\n",
    "        self.test = (self.test-self.nor_mean)/self.nor_std\n",
    "\n",
    "        # add a bias column\n",
    "        self.train.insert(0, \"bias\", numpy.ones(self.train.shape[0]), allow_duplicates=True)\n",
    "        self.test.insert(0, \"bias\", numpy.ones(self.test.shape[0]), allow_duplicates=True)\n",
    "\n",
    "        # split the target column\n",
    "        train_x = self.train.iloc[:, 0:self.train.shape[1]-1]\n",
    "        train_y = self.train.iloc[:, self.train.shape[1]-1]\n",
    "\n",
    "        # store cost value of each iteration of the training\n",
    "        self.costarray = []\n",
    "        m = train_x.shape[0]\n",
    "\n",
    "        # closed form solution\n",
    "        if self.mode == LRMode.CLOSED_FORM_SOLUTION:\n",
    "            # Solve system of linear equation instead of calculating matrix invert (essentially the same thing)\n",
    "            # (X'*X + reg*I)*w = X'*Y\n",
    "            self.w = numpy.linalg.solve(numpy.dot(train_x.T, train_x) + numpy.identity(train_x.shape[1])*self.regularization, numpy.dot(train_x.T, train_y))\n",
    "            y_pred = numpy.dot(train_x, self.w)\n",
    "            residuals = y_pred - train_y\n",
    "            # cost is sum square error + regularization\n",
    "            cost = numpy.sum((residuals ** 2)) + self.regularization * numpy.dot(self.w.T, self.w)\n",
    "            self.costarray.append(cost)            \n",
    "            return\n",
    "\n",
    "        # gd\n",
    "        if self.mode == LRMode.GRADIENT_DESCENT:\n",
    "            # w is initiated as 0\n",
    "            self.w = numpy.zeros(train_x.shape[1])        \n",
    "            last_cost = 0\n",
    "            for i in range(self.n_iteration):\n",
    "                y_pred = numpy.dot(train_x, self.w)\n",
    "                residuals = y_pred - train_y\n",
    "                # gradient are individual for each coefficient, i.e. vector of gradients\n",
    "                gradient = numpy.dot(train_x.T, residuals) + self.w*self.regularization*2\n",
    "                # w = w' - learning_rate*gradient\n",
    "                self.w -= self.learning_rate * gradient\n",
    "                # cost is sum square error + regularization\n",
    "                cost = numpy.sum((residuals ** 2)) + self.regularization * numpy.dot(self.w.T, self.w)\n",
    "                # break loop if change in cost function is negligible\n",
    "                if abs(last_cost - cost) < self.tolerance:\n",
    "                    print (f'Training ended due to negligible improvement, iteration = {i}')\n",
    "                    break\n",
    "                last_cost = cost\n",
    "                self.costarray.append(cost)\n",
    "            return\n",
    "        \n",
    "        # sgd\n",
    "        if self.mode == LRMode.STOCHASTIC_GRADIENT_DESCENT:\n",
    "            # w is initiated as 0\n",
    "            self.w = numpy.zeros(train_x.shape[1])        \n",
    "            last_cost = 0\n",
    "\n",
    "            # In each iteration, shuffle the training set, w is updated after every minibatch\n",
    "\n",
    "            # Big iteration/epoch\n",
    "            for i in range(self.n_iteration):\n",
    "                shuffle_x = train_x.sample(frac=1)\n",
    "                shuffle_y = train_y.reindex(shuffle_x.index)\n",
    "\n",
    "                # Minibatch iterating\n",
    "                index = 0\n",
    "                index_u = 0\n",
    "                br = False\n",
    "                while (br==False):\n",
    "                    if (index + self.minibatch > train_x.shape[0]):\n",
    "                        br = True\n",
    "                        index_u = train_x.shape[0]\n",
    "                    else:\n",
    "                        index_u = index + self.minibatch\n",
    "\n",
    "                    batch_x = shuffle_x.iloc[index:index_u]\n",
    "                    batch_y = shuffle_y.iloc[index:index_u]\n",
    "\n",
    "                    y_pred = numpy.dot(batch_x, self.w)\n",
    "                    residuals = y_pred - batch_y\n",
    "                    # gradient are individual for each coefficient, i.e. vector of gradients\n",
    "                    gradient = numpy.dot(batch_x.T, residuals) + self.w*self.regularization*2\n",
    "                    # w = w' - learning_rate*gradient\n",
    "                    self.w -= self.learning_rate * gradient\n",
    "\n",
    "                    index = index_u\n",
    "\n",
    "                y_pred = numpy.dot(shuffle_x, self.w)\n",
    "                residuals = y_pred - shuffle_y         \n",
    "\n",
    "                # cost is sum square error + regularization\n",
    "                cost = numpy.sum((residuals ** 2)) + self.regularization * numpy.dot(self.w.T, self.w)\n",
    "                # break loop if change in cost function is negligible\n",
    "                if abs(last_cost - cost) < self.tolerance:\n",
    "                    print (f'Training ended due to negligible improvement, iteration = {i}')\n",
    "                    break\n",
    "                last_cost = cost\n",
    "                self.costarray.append(cost)            \n",
    "            return\n",
    "\n",
    "    \n",
    "    # def predict(self, x):\n",
    "\n",
    "    #     return numpy.dot(x, self.w)\n",
    "\n",
    "    #def copy(self):\n",
    "    #    return LinearRegression(self.learning_rate, self.n_iteration)\n",
    "    #__copy__ = copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(a) Housing: learning rate = 0.4 × 10−3, tolerance = 0.5 × 10−2\n",
    "\n",
    "(b) Yacht: learning rate = 0.1 × 10−2, tolerance = 0.1 × 10−2\n",
    "\n",
    "(c) Concrete: learning rate = 0.7 × 10−3, tolerance = 0.1 × 10−3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(mode=LRMode.CLOSED_FORM_SOLUTION, regularization=0.1, learning_rate=0.0004, n_iteration=1000, tolerance=0.005)\n",
    "model.fit(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight:\n",
      "[ 6.46588470e-16 -1.00716877e-01  1.00680462e-01  1.95283660e-02\n",
      "  9.55889483e-02 -2.45788808e-01  2.70057627e-01  1.68946925e-02\n",
      " -3.39381138e-01  3.02400915e-01 -1.88789386e-01 -2.20532458e-01\n",
      "  7.99813963e-02 -4.57737252e-01]\n",
      "Latest training cost: 112.81512488954846\n",
      "Test set SSE: 20.25136244012497\n",
      "Test set RMSE: 0.4477817984366251\n"
     ]
    }
   ],
   "source": [
    "print(f'Model weight:')\n",
    "print(model.w)\n",
    "\n",
    "if model.mode != LRMode.CLOSED_FORM_SOLUTION:\n",
    "    mlp.plot(model.costarray)\n",
    "\n",
    "print(f'Latest training cost: {model.costarray[len(model.costarray)-1]}')\n",
    "print(f'Test set SSE: {model.SSE(model.test)}')\n",
    "print(f'Test set RMSE: {model.RMSE(model.test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model over each instance's test set is recorded as follow:\n",
    "\n",
    "> Closed form solution\n",
    "\n",
    "|    | Housing | Yacht | Concrete|\n",
    "|----|------|------|-|\n",
    "|SSE |20.25136244012497|16.323522202070187|68.68926588873637|\n",
    "|RMSE|0.4477817984366251|0.5172994535316091|0.5774452686574172|\n",
    "\n",
    "\n",
    "> Gradient Descent\n",
    "\n",
    "|    | Housing | Yacht | Concrete|\n",
    "|----|------|------|-|\n",
    "|SSE |28.817485604767736|23.352912763440262|93.78442287119638|\n",
    "|RMSE|0.5341550707904641|0.6187363214376814|0.6747326794332168|\n",
    "\n",
    "\n",
    "> Stochastic gradient descent\n",
    "\n",
    "|    | Housing | Yacht | Concrete|\n",
    "|----|------|------|-|\n",
    "|SSE |27.35117806513256|20.794298397333613|76.73146199768281|\n",
    "|RMSE|0.5203880698278298|0.5838579773037333|0.6103137105058922|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
